{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqzTJ2MXkxm1dCWtAzM/VN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsa-nut/TU_CN408_GenAI_671/blob/main/HW_4_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW 4 - RAG and Evaluation (10 Points)\n",
        "\n",
        "## DUE: September 24th, 2024. 23:59."
      ],
      "metadata": {
        "id": "jLzRzsMm5iPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## คำอธิบาย\n",
        "\n",
        "- การบ้านนี้ เราจะมาฝึกใช้ RAG จริงๆ และก็จะมา evaluate ผลที่ได้ในหลายรูปแบบ\n",
        "- การบ้านจะประกอบไปด้วย:\n",
        "  - การ สร้าง Vector Database จาก documents ที่ให้มา\n",
        "  - การทดสอบเรื่อง การตัดประโยคเก็บไว้ใน Database\n",
        "  - การทดสอบ Embedding ต่างๆ\n",
        "  - การทดสอบ Retrieval เกี่ยวกับ ค่า Top-k\n",
        "  - การทดสอบ คำตอบ โดยใช้ LLM เป็นคนตรวจสอบ (LLM as a judge)\n",
        "- การบ้านนี้น่าจะยาก ควรเริ่มทำตั้งแต่ต้นๆ เมื่อมีปัญหามาถามได้\n",
        "\n"
      ],
      "metadata": {
        "id": "dnm554R7FgV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "- Data นี้มาจาก Huggingface [RAG Evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation#rag-evaluation)\n",
        "- Data มีสองอัน: 1) อันแรกคือ Documents, 2) อันที่สองคือ คำถามที่จะต้องตอบ"
      ],
      "metadata": {
        "id": "1-xR9DYbHlQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-cloud-aiplatform \"anthropic[vertex]\""
      ],
      "metadata": {
        "id": "Uj-Lx9pFFUFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers datasets langchain\n",
        "!pip3 install pinecone"
      ],
      "metadata": {
        "id": "IIWW0_6wkjEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eweqvCWM4KVN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "documents = pd.read_csv(\"hf://datasets/m-ric/huggingface_doc/huggingface_doc.csv\")\n",
        "training  = pd.read_parquet(\"hf://datasets/m-ric/huggingface_doc_qa_eval/data/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents.head()"
      ],
      "metadata": {
        "id": "tE71r6XhJkMQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Documents มี สอง columns: text กับ source\n",
        "- Text คือ Text ทั้ง บทความ. 1 row = 1 บทความ\n",
        "- มีทั้งหมด 2647 บทความ"
      ],
      "metadata": {
        "id": "51wCGtWAK_KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training.head()"
      ],
      "metadata": {
        "id": "fRcFbRZeJlwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- training มีหลาย column หลักๆ ที่จะใช้ แค่ `question` กับ `answer`\n",
        "- Columns อื่นๆมาจาก การสร้าง training data ซึ่ง training data สร้างจาก LLM โดยให้ LLM ตั้งคำถามจาก context ที่ให้ไป แล้วมีการตรวจสอบว่าคำถามตรงกับ context ไหม ซึ่งการตรวจสอบนั้น ใช้สามค่า standalone_score, relatedness_score, relevance_score. อ่านเพิ่มเติมได้ใน Post ที่ link ด้านบน"
      ],
      "metadata": {
        "id": "fMqIrpsULXQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### สร้าง Data ของเรา\n",
        "- เนื่องจาก documents เป็น data ขนาดใหญ่ ซึ่งจะทำให้เราใช้เวลา embedding และ upend to database นานมาก เราจะตัดออกให้เหลือแค่ 100 documents เท่านั้น\n",
        "- ขั้นแรกเราจะเอา documents ที่มีอยู่ใน training มาก่อน\n",
        "- ขั้นสองเราจะเพิ่ม documents เข้าไปจนครบ 100 documents"
      ],
      "metadata": {
        "id": "5F6ID5DXlvKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_index = documents.index[documents['source'].isin(training['source_doc'])].tolist()\n",
        "\n",
        "doc_size = 100\n",
        "i = 0\n",
        "while len(source_index) < doc_size:\n",
        "  if i not in source_index:\n",
        "    source_index.append(i)\n",
        "  i += 5 # grab every fifth documents not in source_index\n",
        "source_index.sort()\n",
        "\n",
        "small_documents = documents.iloc[source_index]\n",
        "small_documents.head()"
      ],
      "metadata": {
        "id": "UkDOUzt6lSDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Data"
      ],
      "metadata": {
        "id": "rxC8CU9NL5Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "1nKghSPzNbZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เราจะทดสอบ Embeddding สองตัว ดังต่อไปนี้\n",
        "- 'all-MiniLM-L6-v2'\n",
        "- 'BAAI/bge-m3' เป็นตัวคนใช้เยอะ เวลาใช้ SentenceTransformer (See [Documentation](https://huggingface.co/BAAI/bge-m3))\n",
        "\n",
        "เนื่องด้วย documents ที่เราใช้นั้นใหญ่ เราจะใช้ cuda ในการรัน\n",
        "- ไปที่ Runtime > Change runtime type > T4 GPU\n"
      ],
      "metadata": {
        "id": "aKcRSVOSOsrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device != 'cuda':\n",
        "    print('No cuda!! - Embedding time will be very long!!!')\n",
        "\n",
        "mini_embedding =  SentenceTransformer('all-MiniLM-L6-v2',  device=device)\n",
        "bge_embedding =  SentenceTransformer('BAAI/bge-m3',  device=device)"
      ],
      "metadata": {
        "id": "qe_y92BpPqXh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ทดสอบ Embedding"
      ],
      "metadata": {
        "id": "FUkQcQjQP8VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Today is a nice day.'\n",
        "mini_ec = mini_embedding.encode(query)\n",
        "bge_ec = bge_embedding.encode(query)\n",
        "\n",
        "print(mini_ec[:5])\n",
        "print(bge_ec[:5])"
      ],
      "metadata": {
        "id": "O49GXC5LP7-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ],
      "metadata": {
        "id": "3tT4I1S-buf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ในขั้นตอนถัดไปเราจะมาเตรียมข้อมูลให้พร้อม ก่อนที่จะเอาไป upend ไปที่ database\n",
        "- ขั้นตอนหลักคือเราจะต้องแบ่งบทความเป็นส่วนย่อๆแทนที่จะใช้ทั้งบทความไป embed\n",
        "- เราจะทดสอบสองแบบ และจะใช้ `Langchain` library มาช่วย\n",
        " - แบบแรกคือ ตัดเป็นความยาวเท่าๆกัน ใช้ `CharacterTextSplitter` ([Documentation](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html))\n",
        " - แบบสองคือ ตัดแบบrecursiveตาม structure ใช้ `RecursiveCharacterTextSplitter` ([Documentation](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html))\n",
        "- parameter ที่สำคัญคือ `chunk` หรือความยาวของประโยคที่จะตัด\n"
      ],
      "metadata": {
        "id": "ydqUoU80bwlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "mJi_UwsfePBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ลองทดสอบกับบทความแรก"
      ],
      "metadata": {
        "id": "n8Ou6l2rekqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_doc = small_documents.text.loc[0]\n",
        "first_doc"
      ],
      "metadata": {
        "id": "FuPBbi3oekXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 50 # Test different numbers\n",
        "char_splitter = CharacterTextSplitter(chunk_size = chunk_size, chunk_overlap=0,\n",
        "                                      separator='', #character that you would like to split on\n",
        "                                      strip_whitespace=True)\n",
        "char_splitter.split_text(first_doc)"
      ],
      "metadata": {
        "id": "TaKKfx-dfg_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Default separators สำหรับ `RecursiveCharacterTextSplitter` นั้นมี แค่ `[\"\\n\\n\", \"\\n\", \" \", \"\"]` เราเลยเพิ่ม `.` ลงไปด้วย"
      ],
      "metadata": {
        "id": "dcJN2f5li-DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 50 # Test different numbers\n",
        "recur_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size,\n",
        "                                                chunk_overlap=20,\n",
        "                                                strip_whitespace=True,\n",
        "                                                separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "                                                )\n",
        "recur_splitter.split_text(first_doc)"
      ],
      "metadata": {
        "id": "lIQrVNgjgIlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- สามารถทดสอบดูได้ที่ web นี้ https://chunkviz.up.railway.app/\n",
        "- ขนาด `chunk_size` ประมาณ 200 - 400 กำลังดี\n"
      ],
      "metadata": {
        "id": "5sxVz9rFhpL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating splitted document (0.5 points)\n",
        "- ถัดไปเราจะสร้างจริง เราจะตัดบทความทั้งหมดและเก็บไว้ใน list สองแบบ แบบแรกคือ ใช้ `CharacterTextSpliter` แบบสองคือ `RecursiveTextSpliter`\n",
        "- `chunk_size` เราจะตั้งไว้ที่ 250\n",
        "- **หมายเหตุ**: สำหรับการบ้านนี้จะทำแบบง่ายๆ สิ่งที่ควรจะทำอีกอย่างอื่น เก็บข้อมูลไว้ด้วยว่า ประโยคที่ถูกตัดออกมาจากบทความไหน"
      ],
      "metadata": {
        "id": "z00CYfHJiosO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_docs = {'char': [],\n",
        "                 'recur':[]}\n",
        "\n",
        "chunk_size = 250\n",
        "char_splitter = ...\n",
        "recur_splitter = ...\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCj-J3lmifuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splitted_docs['char']) # 4701"
      ],
      "metadata": {
        "id": "xqI6rUSMq57R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splitted_docs['recur']) # 7115"
      ],
      "metadata": {
        "id": "XuNWzIoXq8yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Embedded Documents (0.5 points)\n",
        "\n",
        "- ขั้นตอนถัดไป เราจะมาสร้าง Embedded Documents สำหรับแต่ละ splited docs (`char` and `recur`) และ embedding (`mini`, `bge`)\n",
        "- เพราะฉะนั้นจะมีด้วยกันทั้งหมด สี่อัน\n",
        "- Code น่าจะรันนาน โดยเฉพาะถ้าคุณไม่ได้ใช้ GPU"
      ],
      "metadata": {
        "id": "gW9Y30KIrHXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs = { 'mini-char' : ...,\n",
        "                  'mini-recur': ...,\n",
        "                  'bge-char'  : ...,\n",
        "                  'bge-recur' : ...}"
      ],
      "metadata": {
        "id": "NnSIh3pZucUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone Database (2 points)\n",
        "- ในสวนนี้เราจะสร้าง pinecone database\n",
        "- ขั้นแรกคุณจะต้องไปสมัครและเอา api มาใส่ให้เรียบร้อย\n",
        "- pinecone webiste: https://www.pinecone.io/"
      ],
      "metadata": {
        "id": "wcOe6DlBXe-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "id": "Plx29H3bXkdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get secret key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "OAyUqSfGckws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- สร้าง 4 index สำหรับ 4 embedded_docs ที่เราสร้างไว้ และเก็บไว้ใน dict `indexes`"
      ],
      "metadata": {
        "id": "noSkjmGleD8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeds = {'mini': mini_embedding, 'bge': bge_embedding}"
      ],
      "metadata": {
        "id": "j_ges3VBe60M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone = Pinecone(api_key=userdata.get('pinecone_key'))\n",
        "\n",
        "INDEX_NAME = 'hw4-rag'\n",
        "# Store the index in the dict\n",
        "indexes = {}\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "ASTQVJrDcmlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code: สำหรับสร้างเสร็จแล้วไม่ต้องสร้างใหม่\n",
        "# code นี้สำหรับเวลาคุณสร้างเสร็จแล้ว ไม่ต้องสร้างใหม่อีก\n",
        "embedded_list = ['mini-char', 'mini-recur', 'bge-char','bge-recur' ]\n",
        "pinecone = Pinecone(api_key=userdata.get('pinecone_key'))\n",
        "\n",
        "INDEX_NAME = 'hw4-rag'\n",
        "indexes = {}\n",
        "for doc in embedded_list:\n",
        "  indexes[doc] = pinecone.Index(INDEX_NAME + doc)"
      ],
      "metadata": {
        "id": "emfTTj1QHM0P",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- หลังจากสร้างเสร็จ เอาข้อมูลไปใส่บน database ตามที่สร้างไว้\n",
        "- ให้อัพโหลดที่ละ 200 batch ต่อครั้ง อาจจะใช้เวลานานหน่อย\n",
        "- metadatas ที่ต้องใส่คือ `text` หรือข้อความจริง"
      ],
      "metadata": {
        "id": "9iWmwlEFgc9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "nqEL83Lug-S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ตรวจสอบ pinecone database ว่าเรียบร้อยก่อนจะไปต่อ\n",
        "- code ข้างบนควรรันแค่ครั้งเดียว หลังจากนั้นคุณไม่จำเป็นต้อง upend อีก เรียกใช้ได้เลย"
      ],
      "metadata": {
        "id": "8_ZJyupcjD_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation\n",
        "- ถัดไป เราก็จะมาทำเรื่อง retrieval กัน"
      ],
      "metadata": {
        "id": "djaPbQeBwjx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Google Vertex AI\n",
        "- ขั้นแรก คือสร้าง function มาเรียกใช้งาน claude"
      ],
      "metadata": {
        "id": "vzOLTc7iEnTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "yPS6KWarFJ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default set-quota-project gen-ai-demo-3 # replace the last one with your project ID"
      ],
      "metadata": {
        "id": "Ar7kMshCFYf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import AnthropicVertex\n",
        "\n",
        "project_id = \"gen-ai-demo-3\" # replace this with your project ID\n",
        "region = \"us-east5\"  # Two region for Sonnet 3.5 [\"us-east5\", \"europe-west1\"]\n",
        "\n",
        "client = AnthropicVertex(project_id=project_id, region=region)\n",
        "\n",
        "# A simple Q&A generate code\n",
        "def generate(prompt):\n",
        "  response = client.messages.create(\n",
        "    model=\"claude-3-haiku@20240307\",\n",
        "    max_tokens=1000,\n",
        "    messages=[ { \"role\": \"user\", \"content\": prompt}]\n",
        "  )\n",
        "  return response.content[0].text\n",
        "\n",
        "generate(\"Hello test test\")"
      ],
      "metadata": {
        "id": "5YRVMoEHFZh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG function (2 points)\n",
        "- ในช่อง code ด้านล่างให้สร้าง function ที่รับ คำถาม, embedding model, database index, top_k\n",
        "- ขั้นแรก ไปหาว่า documents ที่ใกล้คำถามที่สุดคืออะไร\n",
        "- ขั้นสอง เอา documents ที่ได้มาสร้าง prompt เพื่อตอบคำถาม\n",
        "- ขั้นสาม เอา prompt ไป gen response แล้วก็ return response ออกมา"
      ],
      "metadata": {
        "id": "5m7J8vRbFZ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG_response(query, model, index, top_k=1):\n",
        "  pass\n",
        "\n",
        "# ทดสอบ\n",
        "query = training['question'][0]\n",
        "RAG_response(query, embeds['bge'], indexs['bge-char'])"
      ],
      "metadata": {
        "id": "2VbDNs82wmb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation (5 points)\n",
        "- สิ่งที่เราต้องการทดสอบมีด้วยกันทั้งหมด 3 อย่าง แต่ละอย่างมี สองค่า\n",
        "  - embedding model: `mini` หรือ `bge`\n",
        "  - spliting method: `char` หรือ `recur`\n",
        "  - top-k: `1` หรือ `5`\n",
        "- ดังนั้นจะมีทั้งหมด 8 ค่า เราจะสร้างรูปมาวนตรวจสอบและเก็บค่าของทั้งหมดไว้ใน `DataFrame` (หรือ `dict`) ดังนั้น `DataFrame` นี้จะมี 4 columns: `embedding, spliting, top-k, score` และมี 8 rows\n",
        "- เราใช้ training มาตรวจสอบ\n",
        "- ถัดไปในการ eval นี้ เราจะใช้ LLM มาตรวจว่าดีขนาดไหน\n",
        "- สิ่งสำคัญคือ prompt. prompt ที่จะให้สร้างมีข้อกำหนดดังนี้\n",
        "  - เราจะตรวจสองแค่สิ่งเดียวคือ ความถูกต้อง\n",
        "  - คะแนนที่ได้จาก prompt จะต้องเป็นตัวเลข 0 - 4. 0 คือน้อยสุด (ไม่ถูกต้องเลย) 4 คือมากสุด (ถูกต้องครบถ้วน)\n",
        "- หลังจากตรวจครบแล้วให้หาคะแนนเฉลี่ย และเก็บค่านั้นไว้ ใน column `score` ของแต่ละอัน.\n",
        "- สุดท้าย print `DataFrame` (หรือ `dict`) ออกมา แบบไหนทำได้ดีที่สุด?\n",
        "- **หมายเหตุ 1**: เราใช้ Claude 3 haiku สำหรับการบ้านนี้ ผมลัพท์ที่ได้อาจจะไม่ดีเท่าไร\n",
        "- **หมายเหตุ 2**: ผลลัพท์ที่ได้เราควรจะเห็น ว่า\n",
        "  - `bge` ดีกว่า `mini`\n",
        "  - `recur` ควรจะดีกว่า `char`\n",
        "  - `top-k = 5` ควรจะดีกว่า `top-k = 1`.\n",
        "  - อาจจะไม่มาก และขึ้นอยู่กับ prompt ของคุณ\n",
        "  \n"
      ],
      "metadata": {
        "id": "zmqREFBuwmrT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJYCCULTwooW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
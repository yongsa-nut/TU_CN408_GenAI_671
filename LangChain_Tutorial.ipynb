{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND6X1/kC2xe/DcPiqEsYQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsa-nut/TU_CN408_GenAI_671/blob/main/LangChain_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Tutorial\n",
        "\n",
        "Material is based on https://www.youtube.com/watch?v=yF9kGESAi3M and https://python.langchain.com/docs/tutorials/chatbot/"
      ],
      "metadata": {
        "id": "F2Cl6a6HGTVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Basic"
      ],
      "metadata": {
        "id": "xm_oDLklHGXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQEO1B6oGOnq"
      },
      "outputs": [],
      "source": [
        "#Colab already have langchain preinstalled\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-anthropic"
      ],
      "metadata": {
        "id": "ZIOPRHJMH7lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain supports many different language models\n",
        "\n",
        "1) Claude Example\n",
        "\n",
        "https://python.langchain.com/docs/integrations/chat/anthropic/"
      ],
      "metadata": {
        "id": "69nN9vYbI0i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('anthropic')\n",
        "\n",
        "sonnet = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0)"
      ],
      "metadata": {
        "id": "6QoZmwF0HrAa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sonnet.invoke(\"Hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd1Z5yUNIZ_1",
        "outputId": "a78701f3-992d-457e-f42a-dd336b39ea79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi! How can I help you today?', additional_kwargs={}, response_metadata={'id': 'msg_01Ej6byqJYXFWUBDVvWNEeSe', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 8, 'output_tokens': 17}}, id='run-2189a071-8224-4021-ad45-d37a71ff687b-0', usage_metadata={'input_tokens': 8, 'output_tokens': 17, 'total_tokens': 25, 'input_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sonnet.invoke(\"Hello\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RQI5BRUSJSGF",
        "outputId": "9caf01b2-16eb-4e88-d11c-ef6c5ae9badf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) OpenAI Example"
      ],
      "metadata": {
        "id": "R7tvhQGnI9qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNUipPtTIz-J",
        "outputId": "0ea42e2c-d541-4f76-c955-914371bfdc1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai')\n",
        "\n",
        "gpt4o = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "ptdvjyMaJH_u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o.invoke(\"Hello\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "963pjto2JPdb",
        "outputId": "d9518c82-ced2-46f2-993c-483374ff0cc0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Conversation"
      ],
      "metadata": {
        "id": "FGfBF0PpKXpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Thai\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "sonnet.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tdz8TqE6KeEb",
        "outputId": "cf47fe74-bc0d-46c0-85b5-8184e9c22888"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'สวัสดีค่ะ/ครับ! (sawasdee ka/krap!)\\n\\nNote: ค่ะ (ka) is used by female speakers, ครับ (krap) is used by male speakers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qaSJolTJKloQ",
        "outputId": "122e4d2c-fd3f-419d-88f5-5ad2c76fc7b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'สวัสดี!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi turn Conversation"
      ],
      "metadata": {
        "id": "ym-lAouQK2ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Thai\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "    AIMessage(content=\"สวัสดี! (sà-wàt-dii)\"),\n",
        "    HumanMessage(content=\"Thank you\")\n",
        "]\n",
        "\n",
        "sonnet.invoke(messages).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "erU8_gOOK5D1",
        "outputId": "59931907-0482-4b12-818a-70e1b912e570"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ขอบคุณ (khàawp-khun)\\n\\nYou can also say:\\n- ขอบคุณครับ (khàawp-khun khráp) - for men\\n- ขอบคุณค่ะ (khàawp-khun khâ) - for women'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real-Time Conversation"
      ],
      "metadata": {
        "id": "47yXYbNfLfWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "chat_history.append(system_message)\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "  query = input(\"You: \")\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "  result = sonnet.invoke(chat_history)\n",
        "  response = result.content\n",
        "  chat_history.append(AIMessage(content=response))\n",
        "\n",
        "  print(f\"AI: {response}\")\n",
        "\n",
        "print(\"---Message History---\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "VWyME7UILi2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates\n",
        "\n",
        "Prompt templates = Strings with placeholders to be filled in\n",
        "\n",
        "- Template = `\"Generate {joke_number} jokes about {joke_topic}\"`\n",
        "- Input = `{\"joke_number\":3, \"joke_topic\":\"cats\"}`\n",
        "- Output = `\"Generate 3 jokes about cats\"`"
      ],
      "metadata": {
        "id": "wNHOQfr5MPhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.format_messages(topic=\"cats\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlnTiACIMzmF",
        "outputId": "dd1c683d-4fca-489d-9e58-6439a42f8d79"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"topic\":\"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDp6i4A5NUnA",
        "outputId": "130d84bf-6349-4e6e-9d86-c48e63cf1d2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sonnet.invoke(prompt).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U0AkgFVkRU7p",
        "outputId": "19922198-4ccb-46d9-9b91-f729684df4f1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't cats like online shopping?\\n\\nThey prefer a cat-alog! 😺\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple placeholders\n",
        "\n",
        "template_multiple = \"Generate {joke_number} jokes about {joke_topic}\"\n",
        "prompt_template_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "\n",
        "prompt = prompt_template_multiple.invoke({\"joke_number\":3, \"joke_topic\":\"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X1lh_bNNeP5",
        "outputId": "5ab42e27-c97b-4f7f-a9d1-55fe695aa641"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Generate 3 jokes about cats', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sonnet.invoke(prompt).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "r3cqQbe1RcGw",
        "outputId": "6d291a11-2fc1-4014-b269-810385d71011"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are 3 cat jokes:\\n\\n1. What do you call a cat that becomes a priest?\\nA paw-ther!\\n\\n2. Why don't cats like online shopping?\\nThey prefer a cat-alog!\\n\\n3. What do you call a cat that gets anything it wants?\\nPurrsuasive!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates with System and Human Messages (Using Tuples)\n"
      ],
      "metadata": {
        "id": "ip36zQ2cOFFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\",\"Tell me {joke_number} jokes.\")\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\":\"cats\", \"joke_number\":3})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9CstvNiOQgt",
        "outputId": "39794c98-09c0-4144-92f2-176901872242"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a comedian who tells jokes about cats.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sonnet.invoke(prompt).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "-0UZjuWbRir0",
        "outputId": "2c87995f-0337-4e30-d59c-37ba1e286ed3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are 3 cat-themed jokes:\\n\\n1. What do you call a cat that becomes a priest?\\nA paw-ther!\\n\\n2. Why don't cats like online shopping?\\nThey prefer a cat-alog!\\n\\n3. What's a cat's favorite dessert?\\nA purr-fait!\\n\\n*adjusts microphone and waits for laughs*\\n\\nThese are just a small sample from my purr-sonal collection. I'll be here all week, folks! Remember to tip your waitstaff, and don't forget to spay and neuter your pets!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sonnet.invoke(prompt_template.invoke({\"topic\":\"Thai\",\"joke_number\":3})).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4_LFCy-hRqNz",
        "outputId": "4b6dc635-514c-49f0-831e-52708fad0909"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are 3 Thai-themed jokes:\\n\\n1. Why don\\'t Thai people need alarm clocks?\\nBecause they have a Thai-ming system!\\n\\n2. What do you call a confused Thai person in a restaurant?\\nPad-Thai-ed and confused!\\n\\n3. What did the Thai chef say when he ran out of noodles?\\n\"This is a Pad situation!\"\\n\\n*Note: These are light-hearted jokes meant to be fun and not offensive to Thai culture or people.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain\n",
        "\n",
        "- Chain multiple invokable functions together in one command\n",
        "- LangChain Expression Language (LECL)"
      ],
      "metadata": {
        "id": "qOcwjL3mR6I2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Chain"
      ],
      "metadata": {
        "id": "SjQ9OktZSM24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\",\"Tell me {joke_number} jokes.\")\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "chain = prompt_template | sonnet\n",
        "\n",
        "result = chain.invoke({\"topic\":\"cats\", \"joke_number\":3})\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOTmhKekSOHB",
        "outputId": "2de435c5-4d77-45e4-9ea4-99807303d674"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 cat-themed jokes:\n",
            "\n",
            "1. What do you call a cat that becomes a priest?\n",
            "A paw-ther!\n",
            "\n",
            "2. Why don't cats like online shopping?\n",
            "They prefer a cat-alog!\n",
            "\n",
            "3. What did the cat say when she lost all her money gambling?\n",
            "I'm not feline lucky today!\n",
            "\n",
            "*adjusts microphone and waits for laughs*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain provides `StrOutputParser` function"
      ],
      "metadata": {
        "id": "I3xFyYnsTFfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "chain = prompt_template | sonnet | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"topic\":\"cats\", \"joke_number\":3})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaAvVWP0S8Bj",
        "outputId": "bcba0309-7c02-427f-9f95-1db72806fac4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 cat-themed jokes:\n",
            "\n",
            "1. What do you call a cat that becomes a priest?\n",
            "A paw-ther!\n",
            "\n",
            "2. Why don't cats like online shopping?\n",
            "They prefer a cat-alog!\n",
            "\n",
            "3. What did the cat say when he lost all his money gambling?\n",
            "I'm not feline lucky today!\n",
            "\n",
            "*adjusts microphone and waits for laughs* \n",
            "Hey, tough crowd tonight! These jokes are purr-fect, if you ask me!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Under the hood\n",
        "- You can create your own runnable functions (lambda)."
      ],
      "metadata": {
        "id": "2DQAEPQpTOZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "invoke_model = RunnableLambda(lambda x: sonnet.invoke(x.to_messages()))\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "chain = RunnableSequence(first = format_prompt, middle= [invoke_model], last= parse_output)\n",
        "\n",
        "result = chain.invoke({\"topic\":\"cats\", \"joke_number\":3})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrDieYs7TVmz",
        "outputId": "02fa3f09-d121-41a7-f9b6-7e255f8f7f06"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 cat-themed jokes:\n",
            "\n",
            "1. What do you call a cat that becomes a priest?\n",
            "A paw-ther!\n",
            "\n",
            "2. Why don't cats like online shopping?\n",
            "They prefer a cat-alog!\n",
            "\n",
            "3. What did the cat say when he lost all his money at the casino?\n",
            "I'm not feline lucky today!\n",
            "\n",
            "*adjusts microphone* \n",
            "Hey, if you think those were bad, you should hear my jokes about cat food - they're even worse! They really stink! *winks*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain: Extended"
      ],
      "metadata": {
        "id": "DFBYgbJdUfE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\",\"Tell me {joke_number} jokes.\")\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
        "\n",
        "chain = prompt_template | sonnet | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "result = chain.invoke({\"topic\":\"cats\", \"joke_number\":3})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buFQHYWQUimU",
        "outputId": "fa68ef74-561d-4476-d429-a258b186a39c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count: 77\n",
            "HERE ARE 3 CAT-THEMED JOKES:\n",
            "\n",
            "1. WHAT DO YOU CALL A CAT THAT BECOMES A PRIEST?\n",
            "A PAW-THER!\n",
            "\n",
            "2. WHY DON'T CATS LIKE ONLINE SHOPPING?\n",
            "BECAUSE THEY PREFER A CAT-ALOG!\n",
            "\n",
            "3. WHAT DO YOU CALL A CAT THAT GETS CAUGHT IN THE RAIN?\n",
            "A DROWNED PURR-OUSE!\n",
            "\n",
            "*ADJUSTS MICROPHONE AND WAITS FOR LAUGHS* \n",
            "\n",
            "THESE ARE JUST A SMALL SAMPLE FROM MY PURR-FESSIONAL REPERTOIRE. I'LL BE HERE ALL WEEK, FOLKS! REMEMBER TO TIP YOUR WAITSTAFF, AND TRY THE CATNIP!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain: Parallel\n",
        "\n",
        "- You can create chains with branching"
      ],
      "metadata": {
        "id": "0I7NHIDyVEqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "# Product Review LLM\n",
        "## Step:\n",
        "## Get the features\n",
        "## Two branching: 1) Pros and 2) Cons\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are an expert product reviewer.\"),\n",
        "        (\"human\",\"List the main features of the product {product_name}.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "def analyze_pros(features):\n",
        "  pros_template = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\",\"You are an expert product reviewer.\"),\n",
        "          (\"human\",\"Given these features: {features}, list the pros of these features\")\n",
        "      ]\n",
        "  )\n",
        "  return pros_template.format_prompt(features=features)\n",
        "\n",
        "def analyze_cons(features):\n",
        "  cons_template = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\",\"You are an expert product reviewer.\"),\n",
        "          (\"human\",\"Given these features: {features}, list the cons of these features\")\n",
        "      ]\n",
        "  )\n",
        "  return cons_template.format_prompt(features=features)\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch = RunnableLambda(lambda x: analyze_pros(x)) | sonnet | StrOutputParser()\n",
        "cons_branch = RunnableLambda(lambda x: analyze_cons(x)) | sonnet | StrOutputParser()\n",
        "\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | sonnet\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch, \"cons\": cons_branch})\n",
        "    | RunnableLambda(lambda x: x[\"branches\"][\"pros\"] + \"\\n\\n\" + x[\"branches\"][\"cons\"])\n",
        ")\n",
        "\n",
        "print(chain.invoke({\"product_name\":\"iPhone 15\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeaOfMyOVOMz",
        "outputId": "ccdf89c4-0f21-4a4f-f921-b232be965bbf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the key pros of the iPhone 15's features:\n",
            "\n",
            "Display Pros:\n",
            "- Dynamic Island offers more intuitive notifications and live activities\n",
            "- High-resolution OLED display provides excellent color accuracy and contrast\n",
            "- Sharp resolution ensures crisp text and images\n",
            "- Bright screen suitable for outdoor use\n",
            "\n",
            "Camera System Pros:\n",
            "- 48MP main camera enables higher detail capture and better low-light performance\n",
            "- Improved Portrait mode for better depth effects and subject isolation\n",
            "- Enhanced Night mode for superior low-light photography\n",
            "- Versatile ultra-wide lens for landscape and architectural shots\n",
            "- High-quality front camera for selfies and video calls\n",
            "\n",
            "Performance Pros:\n",
            "- A16 Bionic chip offers powerful performance for apps and games\n",
            "- 6GB RAM ensures smooth multitasking\n",
            "- Multiple storage options to suit different needs\n",
            "- Capable of handling demanding tasks and future iOS updates\n",
            "\n",
            "Design & Build Pros:\n",
            "- Universal USB-C port compatibility with many devices and accessories\n",
            "- Durable aerospace-grade aluminum construction\n",
            "- Ceramic Shield offers excellent drop protection\n",
            "- IP68 rating provides strong water and dust resistance\n",
            "- Modern, sleek design aesthetic\n",
            "\n",
            "Battery & Charging Pros:\n",
            "- All-day battery life reduces charging anxiety\n",
            "- Fast charging capability is convenient for quick power-ups\n",
            "- Multiple charging options (MagSafe, Qi, USB-C)\n",
            "- MagSafe enables convenient magnetic accessory attachment\n",
            "\n",
            "Additional Feature Pros:\n",
            "- Latest iOS 17 with new features and security updates\n",
            "- Emergency SOS via satellite provides safety in remote areas\n",
            "- Crash Detection adds an extra layer of security\n",
            "- 5G connectivity enables fast mobile data speeds\n",
            "- Face ID offers secure and convenient authentication\n",
            "- Diverse color options to suit personal preferences\n",
            "\n",
            "These pros make the iPhone 15 a compelling upgrade, especially for users coming from older iPhone models or those seeking a premium smartphone experience with modern features.\n",
            "\n",
            "Here are the notable cons/drawbacks of the iPhone 15's features:\n",
            "\n",
            "Display:\n",
            "- Dynamic Island still takes up screen real estate\n",
            "- No ProMotion (120Hz refresh rate) like Pro models\n",
            "- Not as bright as Pro models in peak brightness\n",
            "\n",
            "Camera System:\n",
            "- No telephoto lens for optical zoom\n",
            "- Basic 48MP may not fully utilize the sensor's capabilities without Pro features\n",
            "- Video features are more limited compared to Pro models\n",
            "- Portrait mode still struggles in low light conditions\n",
            "\n",
            "Performance:\n",
            "- A16 chip, while powerful, is last year's processor (Pro models get A17)\n",
            "- 6GB RAM might limit future-proofing for heavy multitasking\n",
            "- Base 128GB storage might be insufficient for heavy users\n",
            "- No expandable storage option\n",
            "\n",
            "Design & Build:\n",
            "- USB-C transition means existing Lightning accessories need adapters\n",
            "- Aluminum frame less durable than Pro models' titanium\n",
            "- Still has relatively large bezels compared to some competitors\n",
            "- No major design changes from iPhone 14\n",
            "\n",
            "Battery & Charging:\n",
            "- Charging speeds still lag behind Android competitors\n",
            "- No charger included in the box\n",
            "- MagSafe charging is slower than wired charging\n",
            "- Battery life might be insufficient for power users\n",
            "\n",
            "Other Features:\n",
            "- No Always-On Display\n",
            "- Limited customization options compared to Android\n",
            "- Emergency SOS and Crash Detection can trigger false alarms\n",
            "- Color options might be too subtle/muted for some users\n",
            "- Price remains premium despite using last-gen technology\n",
            "\n",
            "General Ecosystem Issues:\n",
            "- Locked into Apple ecosystem\n",
            "- Limited third-party repair options\n",
            "- Expensive official repairs\n",
            "- Restricted sideloading of apps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain : Multiple Branching"
      ],
      "metadata": {
        "id": "phzlD-QjXlNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableBranch\n",
        "\n",
        "# Product Review LLM\n",
        "## Step:\n",
        "## Get the features\n",
        "## Two branching: 1) Pros and 2) Cons\n",
        "\n",
        "# Content Analysis\n",
        "\n",
        "positive_feedback_tempalte = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant.\"),\n",
        "        (\"human\",\"Generate a thank you note for this positive feedback: {feedback}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_tempalte = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant.\"),\n",
        "        (\"human\",\"Generate a response addressing this negative feedback: {feedback}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_tempalte = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant.\"),\n",
        "        (\"human\",\"Generate a request for more details for this neutral feedback: {feedback}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant.\"),\n",
        "        (\"human\",\"Generate a message to esclate this feedback to a human agent: {feedback}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify this feedback into one of the following categories: positive, negative, neutral, or escalate: {feedback}.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "  (\n",
        "      lambda x: 'positive' in x,\n",
        "      positive_feedback_tempalte | sonnet | StrOutputParser()\n",
        "  ),\n",
        "  (\n",
        "      lambda x: 'negative' in x,\n",
        "      negative_feedback_tempalte | sonnet | StrOutputParser()\n",
        "  ),\n",
        "  (\n",
        "      lambda x: 'neutral' in x,\n",
        "      neutral_feedback_tempalte | sonnet | StrOutputParser()\n",
        "  ),\n",
        "  escalate_feedback_template | sonnet | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classfication_chain = classification_template | sonnet | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classfication_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "good_review = \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "bad_review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "neutral_review = \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "default = \"I'm not sure about the product yet. Can you tell me more about its features and benefits.\"\n",
        "\n",
        "result = chain.invoke({'feedback': good_review})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0AbgyaRXOEe",
        "outputId": "749b3487-06b6-46bc-a196-a67e349f3dd8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a thank you note for the positive feedback:\n",
            "\n",
            "Dear Valued Customer,\n",
            "\n",
            "Thank you so much for taking the time to share your wonderful feedback! We are thrilled to hear that you found our product to be excellent and very helpful. It brings us great joy to know that you really enjoyed your experience with us.\n",
            "\n",
            "Your positive comments mean a lot to our team and validate our commitment to delivering quality products that make a difference for our customers. Feedback like yours motivates us to maintain our high standards and continue improving.\n",
            "\n",
            "Thank you again for your support and kind words. We look forward to serving you again in the future!\n",
            "\n",
            "Best regards,\n",
            "[Company Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with LangChain\n",
        "- Material from https://python.langchain.com/docs/integrations/vectorstores/pinecone/\n",
        "- Note: Their docs is not good..."
      ],
      "metadata": {
        "id": "h6MLJY5KcjXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-pinecone pinecone-notebooks"
      ],
      "metadata": {
        "id": "OWaF4KefczRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_api_key = userdata.get('pinecone_key')\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "AOqtyM-QclJU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"langchain-test-index\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "AZeL3_kkiYtT"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Embedding"
      ],
      "metadata": {
        "id": "BhIFhzkYihcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-huggingface"
      ],
      "metadata": {
        "id": "rkjZvxJNifUR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "CR6kHY1tilDM"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "cyU0okCKime5"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manage Vector Store"
      ],
      "metadata": {
        "id": "VZV6KgASjpAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add items to vector store"
      ],
      "metadata": {
        "id": "cXE_3jb4jrgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoTceiwUjtc2",
        "outputId": "0daecc75-a91f-4fdc-85fc-958f68309c10"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f680db97-e7f0-4844-bd03-855a3079d0de',\n",
              " 'cee544e7-d843-4203-b5c6-bdc2c1d693f3',\n",
              " '5ad8baf7-d752-43b8-8650-6dcca2689aa9',\n",
              " 'ec503d02-e094-422d-92ff-164af2055e79',\n",
              " 'fbc5ca37-2f0a-4446-afbe-6e67251c0a93',\n",
              " 'fb45a5de-12f9-4bea-9d6a-c15ebede3509',\n",
              " 'c7bad5e5-e261-4075-bf86-da7ec36f90f6',\n",
              " '7985c3e1-acb9-466b-b408-cd11f640c2f3',\n",
              " '31b3a6ba-7d8d-4019-9627-284c26822fee',\n",
              " '36d96b1d-5195-48c8-93c9-2ca330ee5ce0']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query vector store"
      ],
      "metadata": {
        "id": "ovYbLS2ukYq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query directly"
      ],
      "metadata": {
        "id": "K_Hr6EkJkb9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kux45RP9kd4u",
        "outputId": "5427a99c-1b7a-4643-818b-ddaff83d8fe8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity search with score\n",
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ2PvolWkmSt",
        "outputId": "70caf744-a011-4451-b870-291bd6e70afc"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.595264] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query by turning into retriever\n",
        "\n",
        "Transform the vector store into a retriever for easier usage in your chains."
      ],
      "metadata": {
        "id": "eMAotUejksWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n",
        ")\n",
        "retrieved_docs = retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})\n",
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWl0TGm0kxHG",
        "outputId": "b8fe4a5e-f8d7-44cf-9ecc-31c236493c26"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='ec503d02-e094-422d-92ff-164af2055e79', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3C_iqMqplKyf",
        "outputId": "c8947c2e-bb3c-4b34-9535-6bd768aced76"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Robbers broke into the city bank and stole $1 million in cash.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain with retriever"
      ],
      "metadata": {
        "id": "JkiuhYfJlYYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we'll use a prompt for RAG that is checked into the LangChain prompt hub ([here](https://smith.langchain.com/hub/rlm/rag-prompt))."
      ],
      "metadata": {
        "id": "qcy5_p6olzb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQdMCyJRlbDf",
        "outputId": "2d8d92a1-f913-4bb4-a09b-10ff39e6592e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually copy from the hub\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\"\"\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC1E3QWsl-XE",
        "outputId": "c9509cbc-1664-4b5e-8465-e153038f3bc6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\nQuestion: filler question \\n\\nContext: filler context \\n\\nAnswer:\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} # the dict with context and question is cast to a RunnableParallel.\n",
        "    | prompt\n",
        "    | sonnet\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"Will it be hot tomorrow?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnwTU9Zjmb2R",
        "outputId": "eb923f75-8671-4863-f0de-588bd4c3348c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the forecast, it will not be hot tomorrow, with temperatures reaching only 62 degrees Fahrenheit and cloudy conditions. This would be considered mild or cool weather for most regions."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've seen above, the input to `prompt` is expected to be a dict with keys `\"context\"` and `\"question\"`. So the first element of this chain builds runnables that will calculate both of these from the input question:\n",
        "\n",
        "* `retriever | format_docs` passes the question through the retriever, generating Document objects, and then to `format_docs` to generate strings;\n",
        "* `RunnablePassthrough()` passes through the input question unchanged.\n",
        "\n",
        "More info here: https://python.langchain.com/docs/tutorials/rag/"
      ],
      "metadata": {
        "id": "AnWRo422mvxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool Use with LangChain\n",
        "\n",
        "- There are many tools. https://python.langchain.com/v0.1/docs/integrations/tools/"
      ],
      "metadata": {
        "id": "QS9Xmn9XclfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade youtube_search\n",
        "!pip install langchain-community langchain-core"
      ],
      "metadata": {
        "id": "v0BNAtr3coUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Documentation](https://python.langchain.com/api_reference/_modules/langchain_community/tools/youtube/search.html#YouTubeSearchTool)\n",
        "- YoutubeSearch: search for youtube videos associated with a person."
      ],
      "metadata": {
        "id": "7aM7mLTheWm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import YouTubeSearchTool\n",
        "\n",
        "youtube_tool = YouTubeSearchTool()"
      ],
      "metadata": {
        "id": "3nFFTzzndBAs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_tool.run(\"Nutchanon Yongsatianchot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ebjwA5eReEvH",
        "outputId": "c9e7da9a-ca75-4a67-d1ac-ffadf46b09a3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['https://www.youtube.com/watch?v=0ByyrBUcjqQ&pp=ygUYTnV0Y2hhbm9uIFlvbmdzYXRpYW5jaG90', 'https://www.youtube.com/watch?v=juAqT9LqmGI&pp=ygUYTnV0Y2hhbm9uIFlvbmdzYXRpYW5jaG90']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bind the Youtube tool to the LLM\n",
        "llm_with_tools = sonnet.bind_tools([youtube_tool])\n",
        "msg = llm_with_tools.invoke(\"Nutchanon Yongsatianchot Youtube Videos\")\n",
        "print(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfOp6fMmfTeC",
        "outputId": "73b78d25-8d20-46ff-ec00-9953036db7d1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=[{'text': \"I'll help you search for YouTube videos associated with Nutchanon Yongsatianchot.\", 'type': 'text'}, {'id': 'toolu_013B32kjQv5MmqnhkdDQhLR9', 'input': {'query': 'Nutchanon Yongsatianchot'}, 'name': 'youtube_search', 'type': 'tool_use'}] additional_kwargs={} response_metadata={'id': 'msg_01Yb7A3s85WpasHyRWsJeFqV', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 435, 'output_tokens': 89}} id='run-496000eb-f9b3-4e44-b297-429a5017b528-0' tool_calls=[{'name': 'youtube_search', 'args': {'query': 'Nutchanon Yongsatianchot'}, 'id': 'toolu_013B32kjQv5MmqnhkdDQhLR9', 'type': 'tool_call'}] usage_metadata={'input_tokens': 435, 'output_tokens': 89, 'total_tokens': 524, 'input_token_details': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg.tool_calls[0][\"args\"][\"query\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nUeopgXMfcIL",
        "outputId": "f0da359c-fcb3-4a65-fec6-c78702a1659c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nutchanon Yongsatianchot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"][\"query\"]) | youtube_tool\n",
        "print(chain.invoke(\"Find some Youtube Videos about Nutchanon Yongsatianchot\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMYnyg5cgcUz",
        "outputId": "807a7bb1-8384-47fb-c9b7-6e6f265e8a35"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.youtube.com/watch?v=0ByyrBUcjqQ&pp=ygUYTnV0Y2hhbm9uIFlvbmdzYXRpYW5jaG90', 'https://www.youtube.com/watch?v=juAqT9LqmGI&pp=ygUYTnV0Y2hhbm9uIFlvbmdzYXRpYW5jaG90']\n"
          ]
        }
      ]
    }
  ]
}
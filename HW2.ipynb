{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbuMbJC+c2XE3o2w8F6tWD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsa-nut/TU671_CN408_GenAI/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Count Tokens (1 point)\n",
        "\n",
        "Count the number of tokens using [gpt-4 tokenizer](https://platform.openai.com/tokenizer) (you can use the web or the code) of the following four prompts:\n",
        "\n",
        "- The old man sat on the park bench, feeding pigeons and lost in thought. As the sun began to set, he smiled, realizing that sometimes the simplest moments are the most precious.\n",
        "\n",
        "- ชายชรานั่งอยู่บนม้านั่งในสวนสาธารณะ ให้อาหารนกพิราบและจมอยู่ในภวังค์ความคิด เม่ือดวงอาทิตย์เรี่มตกดิน เขายิ้ม ตระหนักว่าบางครั้งช่วงเวลาที่เรียบง่ายที่สุดกลับมีค่าที่สุด\n",
        "\n",
        "- 老人は公園のベンチに座り、鳩に餌をやりながら物思いにふけっていた。日が沈み始めると、彼は微笑み、時として最も単純な瞬間が最も貴重であることに気づいた。\n",
        "\n",
        "- 9.11 > 9.9"
      ],
      "metadata": {
        "id": "ROi0QF4-XpUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSaVGdqvO8nE"
      },
      "outputs": [],
      "source": [
        "# List your answers here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Word Embedding: Closet words (1 point)\n",
        "\n",
        "For this question, your job is find the closest words of the following words in the vector space of Word2Vec embedding.\n",
        "\n",
        "1) cat\n",
        "\n",
        "2) King\n",
        "\n",
        "3) teaching\n",
        "\n",
        "4) learning\n",
        "\n",
        "The code is provided below."
      ],
      "metadata": {
        "id": "Ue7A7YVJZEAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First install the library spacy\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9BeQZpENZJ9W",
        "outputId": "d13be7c1-e349-4da6-b04f-cf6be8dea165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained Word2Vec embeddings (~1.6gb)\n",
        "model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5MJVMEiaXEX",
        "outputId": "e5813379-9bdc-4392-8a83-30c3835e0720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of usage\n",
        "word = 'Merry'\n",
        "model.similar_by_word(word) # It will return top 10 closest words. ['word','cosine similarity']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-55vrGrgLLi",
        "outputId": "748fa543-0585-4425-8045-42d6064542e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Shavaun', 0.49784329533576965),\n",
              " ('Apple_Dumpling', 0.4907097816467285),\n",
              " ('Merry_Merry', 0.4760850667953491),\n",
              " ('Christma', 0.4625037908554077),\n",
              " ('Gravener', 0.4579834043979645),\n",
              " ('OPEN_RESTRICTED_1', 0.45711594820022583),\n",
              " ('Reindeers', 0.4541266858577728),\n",
              " ('Rub_Dub_Dub', 0.4538874328136444),\n",
              " ('Fancy_Nancy_Splendiferous', 0.4518665373325348),\n",
              " ('XMas', 0.45152536034584045)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List your answers here"
      ],
      "metadata": {
        "id": "Ma6e9M7ZhZpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Word Embedding: Analogy (1 point)\n",
        "\n",
        "For this question, you task is to find four analogies of the following form:\n",
        "\n",
        "**man** is to **woman** as ***king*** is to ***queen***.\n",
        "\n",
        "Specifically, your job is to come up with the first three bold words (man, woman, king) and the code below will generate the analogy word (queen) using Word2Vec as discussed in class. See code below for more examples."
      ],
      "metadata": {
        "id": "fb3BCe38ZKbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to run the code to load up the model first from 2)\n",
        "def vector_analogy(word1, word2, word3):\n",
        "    try:\n",
        "        # Get word vectors\n",
        "        vec1 = model[word1]\n",
        "        vec2 = model[word2]\n",
        "        vec3 = model[word3]\n",
        "\n",
        "        # Compute the analogy vector\n",
        "        analogy_vec = vec2 - vec1 + vec3\n",
        "\n",
        "        # Find the most similar word to the analogy vector\n",
        "        result = model.similar_by_vector(analogy_vec, topn=5)\n",
        "\n",
        "        # Filter out input words\n",
        "        for word, similarity in result:\n",
        "            if word not in [word1, word2, word3]:\n",
        "                return word\n",
        "\n",
        "        return \"No suitable word found\"\n",
        "    except KeyError:\n",
        "        return \"One or more words not in vocabulary\"\n",
        "\n",
        "# Example usage\n",
        "analogies = [\n",
        "    (\"man\", \"woman\", \"king\"),\n",
        "    (\"paris\", \"france\", \"rome\"),\n",
        "    (\"big\", \"bigger\", \"small\"),\n",
        "    (\"japan\", \"sushi\", \"italy\")\n",
        "]\n",
        "\n",
        "for w1, w2, w3 in analogies:\n",
        "    result = vector_analogy(w1, w2, w3)\n",
        "    print(f\"{w1} is to {w2} as {w3} is to {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJpzYJsYiNxg",
        "outputId": "a1627301-3ebd-4f7e-cfdf-5d198b968694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man is to woman as king is to queen\n",
            "paris is to france as rome is to italy\n",
            "big is to bigger as small is to larger\n",
            "japan is to sushi as italy is to gourmet_pizza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List your answers here the same as the example code above.\n",
        "analogies = [\n",
        "    ()\n",
        "]"
      ],
      "metadata": {
        "id": "CEG4mGfrKKBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Sentence Embedding: Closest sentences (2 points)\n",
        "\n",
        "For this question, you will be using cohere API to explore Sentence Embedding. The idea is still the same, similar sentences will be closer in the embedding space.\n",
        "\n",
        "To run the code below, the first step you must obtain cohere API trial (free) key from their [website](https://dashboard.cohere.ai/welcome/register).\n",
        "\n",
        "![key_trial.png](https://drive.google.com/uc?export=view&id=1rdMaMdO5S7eaktN0qwodhgQd3auuZ6gi)\n",
        "\n",
        "* Note: Be careful about rate limit for free key. 5 calls per minute."
      ],
      "metadata": {
        "id": "GDiQhzguZPkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "id": "Or293PTsQbY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cohere\n",
        "co = cohere.Client(\"\") # Your Cohere API key"
      ],
      "metadata": {
        "id": "Au3eOYBsQXdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our documents. It is just a list of strings (sentences)\n",
        "# (Taken from https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/VoyageAI/how_to_create_embeddings.md)\n",
        "documents = [\n",
        "    \"The Mediterranean diet emphasizes fish, olive oil, and vegetables, believed to reduce chronic diseases.\",\n",
        "    \"Photosynthesis in plants converts light energy into glucose and produces essential oxygen.\",\n",
        "    \"20th-century innovations, from radios to smartphones, centered on electronic advancements.\",\n",
        "    \"Rivers provide water, irrigation, and habitat for aquatic species, vital for ecosystems.\",\n",
        "    \"Apple’s conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.\",\n",
        "    \"Shakespeare's works, like 'Hamlet' and 'A Midsummer Night's Dream,' endure in literature.\"\n",
        "]\n",
        "\n",
        "# Embedded (More info: https://docs.cohere.com/reference/embed?ref=cohere-ai.ghost.io )\n",
        "response = co.embed(\n",
        "    texts=documents,\n",
        "    model='embed-english-v3.0',\n",
        "    input_type='search_document'\n",
        ")\n",
        "\n",
        "embeddings = response.embeddings\n",
        "\n",
        "print(f\"The dimension of the embedding =  {len(embeddings[0])}\")\n",
        "\n",
        "print(f\"Embedding for sentence 1: {embeddings[0]}\")\n",
        "print(f\"Embedding for sentence 2: {embeddings[1]}\")\n",
        "print(f\"Embedding for sentence 3: {embeddings[0]}\")\n",
        "\n",
        "# To calculate similarity we can use dot product (same as cosine similarity when vectors have been normarlized to 1.)\n",
        "print(\"Similarity between sentences 1 and 2:\", np.dot(embeddings[0], embeddings[1]))\n",
        "print(\"Similarity between sentences 1 and 3:\", np.dot(embeddings[0], embeddings[2]))\n",
        "print(\"Similarity between sentences 2 and 3:\", np.dot(embeddings[0], embeddings[3]))\n",
        "\n",
        "# A similar sentence to the first sentence\n",
        "temp = \"Focusing on seafood, olive oil, and plant-based foods, the Mediterranean eating pattern is thought to lower the risk of long-term health conditions.\"\n",
        "new_embeddings = co.embed(\n",
        "    texts=[temp],\n",
        "    model='embed-english-v3.0',\n",
        "    input_type='search_document'\n",
        ")\n",
        "new_embeddings = new_embeddings.embeddings\n",
        "\n",
        "print(\"Similarity between sentences 1 and the paraphased version:\",\n",
        "      np.dot(embeddings[0], new_embeddings[0]))"
      ],
      "metadata": {
        "id": "Mb0NxJ1_ZUaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for question 4\n",
        "def closest_sentence(sentence, documents, model='embed-english-v3.0'):\n",
        "  # Merge into one so that it would only use one call to API.\n",
        "  documents = documents.copy() + [sentence]\n",
        "  response = co.embed(\n",
        "    texts=documents,\n",
        "    model=model,\n",
        "    input_type='search_document'\n",
        "  )\n",
        "  embeddings = response.embeddings\n",
        "  similarities = [np.dot(embeddings[i], embeddings[-1]) for i in range(len(embeddings)-1)]\n",
        "  # print similarities and index\n",
        "  for i, sim in enumerate(similarities):\n",
        "    print(f\"Similarity between sentence {i+1} and the new sentence: {sim}\")\n",
        "\n",
        "  max_index = np.argmax(similarities)\n",
        "  return max_index, documents[max_index]"
      ],
      "metadata": {
        "id": "JRfYWN8xX5ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1) Find a sentence that is closest to the fifth sentence (\"Apple’s conference call ...\")\n",
        "your_sentence = \"\" #Your sentence here\n",
        "index, sentence = closest_sentence(your_sentence, documents)\n",
        "print(f\"The closest sentence to '{your_sentence}' is '{sentence}' at index {index}.\")"
      ],
      "metadata": {
        "id": "Yb9E3K-8R2Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2) Find a sentence that is closest to the sixth sentence (\"Shakespeare's works ... \")\n",
        "#     The sentence must not contain the word 'Shakespeare'\n",
        "your_sentence = \"\" #Your sentence here\n",
        "index, sentence = closest_sentence(your_sentence, documents)\n",
        "print(f\"The closest sentence to '{your_sentence}' is '{sentence}' at index {index}.\")"
      ],
      "metadata": {
        "id": "gMRvWX9mYHl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3) Find a sentence in \"Thai\" that is closest to the second sentence. (\"Photosynthesis in plants converts ...\")\n",
        "your_sentence = \"\" #Your sentence here\n",
        "index, sentence = closest_sentence(your_sentence, documents, 'embed-multilingual-v3.0')\n",
        "print(f\"The closest sentence to '{your_sentence}' is '{sentence}' at index {index}.\")"
      ],
      "metadata": {
        "id": "TeLxYCteW8sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4) Find a sentence in \"Japan\" that is closest to the first sentence. (\"The Mediterranean  ...\")\n",
        "your_sentence = \"\" #Your sentence here\n",
        "index, sentence = closest_sentence(your_sentence, documents, 'embed-multilingual-v3.0')\n",
        "print(f\"The closest sentence to '{your_sentence}' is '{sentence}' at index {index}.\")"
      ],
      "metadata": {
        "id": "qf1tkXmqYtWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}